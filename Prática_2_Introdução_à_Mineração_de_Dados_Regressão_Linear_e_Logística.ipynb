{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cdiegor/MineracaoDeDados/blob/main/Pr%C3%A1tica_2_Introdu%C3%A7%C3%A3o_%C3%A0_Minera%C3%A7%C3%A3o_de_Dados_Regress%C3%A3o_Linear_e_Log%C3%ADstica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2440acaf",
      "metadata": {
        "id": "2440acaf"
      },
      "source": [
        "\n",
        "# Regressão Linear e Logística\n",
        "**Tópicos:** seleção de atributos *(forward/backward)*, engenharia de atributos com **regressão polinomial**, avaliação com validação cruzada.\n",
        "\n",
        "**Datasets:**  \n",
        "- **Classificação** — *Breast Cancer Wisconsin* (binário)  \n",
        "- **Regressão** — *California Housing* (alvo contínuo)\n",
        "\n",
        "> Objetivo: dominar um ciclo prático de **seleção de atributos** e **engenharia de atributos** evitando vazamento (CV no treino), e comparar modelos baseline vs. versões com seleção/atributos polinomiais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8968c83",
      "metadata": {
        "id": "c8968c83"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Preparação ---------------------------------------------------------------\n",
        "# !pip install scikit-learn pandas numpy matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "np.random.seed(7)\n",
        "plt.rcParams['figure.figsize'] = (7, 4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e429b73c",
      "metadata": {
        "id": "e429b73c"
      },
      "source": [
        "\n",
        "## 1) Dados e divisões\n",
        "Carregaremos dois conjuntos do `scikit-learn` e criaremos *splits* de treino/teste. As seleções por *stepwise* serão **sempre** feitas com **validação cruzada** no **treino**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a17a4d7",
      "metadata": {
        "id": "0a17a4d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Classificação (Breast Cancer)\n",
        "bc = load_breast_cancer(as_frame=True)\n",
        "Xc = bc.data.copy()\n",
        "yc = bc.target.copy()\n",
        "\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    Xc, yc, test_size=0.2, stratify=yc, random_state=42\n",
        ")\n",
        "\n",
        "# Regressão (California Housing)\n",
        "house = fetch_california_housing(as_frame=True)\n",
        "Xr = house.data.copy()\n",
        "yr = house.target.copy()\n",
        "\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
        "    Xr, yr, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "Xc_train.shape, Xr_train.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53807e07",
      "metadata": {
        "id": "53807e07"
      },
      "source": [
        "\n",
        "## 2) Utilitários — Avaliação por CV em subconjuntos de atributos\n",
        "As funções abaixo avaliam um conjunto de colunas usando **Pipeline(StandardScaler → Estimador)** e `cross_val_score`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462e6883",
      "metadata": {
        "id": "462e6883"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cv_score_for_features(X, y, feature_list, estimator, cv, scoring):\n",
        "    \"\"\"Retorna média do score de CV para as colunas em feature_list.\"\"\"\n",
        "    Xsub = X[feature_list]\n",
        "    pipe = Pipeline([('scaler', StandardScaler()), ('est', estimator)])\n",
        "    scores = cross_val_score(pipe, Xsub, y, cv=cv, scoring=scoring)\n",
        "    return scores.mean(), scores.std()\n",
        "\n",
        "def best_next_feature_forward(X, y, current_feats, candidate_feats, estimator, cv, scoring):\n",
        "    best_feat, best_mean, best_std = None, -np.inf, None\n",
        "    for f in candidate_feats:\n",
        "        feats = current_feats + [f]\n",
        "        mean, std = cv_score_for_features(X, y, feats, estimator, cv, scoring)\n",
        "        if mean > best_mean:\n",
        "            best_feat, best_mean, best_std = f, mean, std\n",
        "    return best_feat, best_mean, best_std\n",
        "\n",
        "def worst_feature_backward(X, y, current_feats, estimator, cv, scoring):\n",
        "    \"\"\"Retorna a feature cuja remoção melhora mais (ou piora menos) o score.\"\"\"\n",
        "    baseline_mean, baseline_std = cv_score_for_features(X, y, current_feats, estimator, cv, scoring)\n",
        "    best_drop, best_mean, best_std = None, baseline_mean, baseline_std\n",
        "    improved = False\n",
        "    for f in current_feats:\n",
        "        feats = [c for c in current_feats if c != f]\n",
        "        mean, std = cv_score_for_features(X, y, feats, estimator, cv, scoring)\n",
        "        if mean >= best_mean + 1e-10:\n",
        "            improved = True\n",
        "            best_drop, best_mean, best_std = f, mean, std\n",
        "    return improved, best_drop, best_mean, best_std, baseline_mean, baseline_std\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ce9798",
      "metadata": {
        "id": "d8ce9798"
      },
      "source": [
        "\n",
        "## 3) Seleção *Forward* (adição sequencial)\n",
        "Iterativamente, adiciona a **próxima** variável que mais melhora o `score` de CV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6d1981",
      "metadata": {
        "id": "1b6d1981"
      },
      "outputs": [],
      "source": [
        "\n",
        "def forward_stepwise(X, y, estimator, cv, scoring, max_features=None, verbose=True):\n",
        "    candidates = list(X.columns)\n",
        "    selected = []\n",
        "    history = []\n",
        "    k = 0\n",
        "    while candidates and (max_features is None or k < max_features):\n",
        "        best_feat, best_mean, best_std = best_next_feature_forward(\n",
        "            X, y, selected, candidates, estimator, cv, scoring\n",
        "        )\n",
        "        selected.append(best_feat)\n",
        "        candidates.remove(best_feat)\n",
        "        k += 1\n",
        "        history.append({'k': k, 'added': best_feat, 'cv_mean': best_mean, 'cv_std': best_std})\n",
        "        if verbose:\n",
        "            print(f\"[{k}] + {best_feat:>20s}  -> CV {scoring}: {best_mean:.4f} ± {best_std:.4f}\")\n",
        "    return selected, pd.DataFrame(history)\n",
        "\n",
        "# Exemplo: classificação com ROC-AUC\n",
        "cv_cls = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "selected_fwd_cls, hist_fwd_cls = forward_stepwise(\n",
        "    Xc_train, yc_train, logreg, cv=cv_cls, scoring='accuracy', max_features=8, verbose=True\n",
        ")\n",
        "hist_fwd_cls.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1883ac3f",
      "metadata": {
        "id": "1883ac3f"
      },
      "source": [
        "\n",
        "## 4) Seleção *Backward* (remoção sequencial)\n",
        "Começa com **todas** as variáveis e remove a que **menos contribui**, desde que o `score` **não piore**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a81766e",
      "metadata": {
        "id": "8a81766e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def backward_stepwise(X, y, estimator, cv, scoring, min_features=1, verbose=True):\n",
        "    selected = list(X.columns)\n",
        "    changed = True\n",
        "    history = []\n",
        "    step = 0\n",
        "    while changed and len(selected) > min_features:\n",
        "        changed, drop_feat, mean, std, base_mean, base_std = worst_feature_backward(\n",
        "            X, y, selected, estimator, cv, scoring\n",
        "        )\n",
        "        step += 1\n",
        "        history.append({'step': step, 'dropped': drop_feat, 'cv_mean': mean, 'cv_std': std, 'baseline': base_mean})\n",
        "        if verbose:\n",
        "            print(f\"[{step}] - {str(drop_feat):>20s}  -> CV {scoring}: {mean:.4f} (base {base_mean:.4f})\")\n",
        "        if changed:\n",
        "            selected.remove(drop_feat)\n",
        "    return selected, pd.DataFrame(history)\n",
        "\n",
        "# Exemplo: regressão (RMSE menor é melhor, então usamos neg_mean_squared_error no scoring)\n",
        "cv_reg = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "linreg = LinearRegression()\n",
        "\n",
        "scoring_rmse = 'neg_root_mean_squared_error'\n",
        "selected_bwd_reg, hist_bwd_reg = backward_stepwise(\n",
        "    Xr_train, yr_train, linreg, cv=cv_reg, scoring=scoring_rmse, min_features=4, verbose=True\n",
        ")\n",
        "hist_bwd_reg.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdb1fe48",
      "metadata": {
        "id": "bdb1fe48"
      },
      "source": [
        "\n",
        "## 5) Comparação no *hold-out* (teste)\n",
        "Treinamos modelos com as listas de atributos selecionadas e comparamos no conjunto de **teste**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85620a8e",
      "metadata": {
        "id": "85620a8e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Classificação - baseline vs forward selecionado\n",
        "pipe_base_cls = Pipeline([('scaler', StandardScaler()), ('est', LogisticRegression(max_iter=1000))])\n",
        "pipe_fwd_cls  = Pipeline([('scaler', StandardScaler()), ('est', LogisticRegression(max_iter=1000))])\n",
        "\n",
        "pipe_base_cls.fit(Xc_train, yc_train)\n",
        "auc_base = roc_auc_score(yc_test, pipe_base_cls.predict_proba(Xc_test)[:,1])\n",
        "\n",
        "pipe_fwd_cls.fit(Xc_train[selected_fwd_cls], yc_train)\n",
        "auc_fwd  = roc_auc_score(yc_test, pipe_fwd_cls.predict_proba(Xc_test[selected_fwd_cls])[:,1])\n",
        "\n",
        "print(f\"Classificação (ROC-AUC teste) -> Baseline (todas): {auc_base:.3f} | Forward({len(selected_fwd_cls)} feats): {auc_fwd:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2aab59",
      "metadata": {
        "id": "1a2aab59"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Regressão - baseline vs backward selecionado\n",
        "pipe_base_reg = Pipeline([('scaler', StandardScaler()), ('est', LinearRegression())])\n",
        "pipe_bwd_reg  = Pipeline([('scaler', StandardScaler()), ('est', LinearRegression())])\n",
        "\n",
        "pipe_base_reg.fit(Xr_train, yr_train)\n",
        "pred_base = pipe_base_reg.predict(Xr_test)\n",
        "rmse_base = np.sqrt(mean_squared_error(yr_test, pred_base))\n",
        "r2_base   = r2_score(yr_test, pred_base)\n",
        "\n",
        "pipe_bwd_reg.fit(Xr_train[selected_bwd_reg], yr_train)\n",
        "pred_bwd = pipe_bwd_reg.predict(Xr_test[selected_bwd_reg])\n",
        "rmse_bwd = np.sqrt(mean_squared_error(yr_test, pred_bwd))\n",
        "r2_bwd   = r2_score(yr_test, pred_bwd)\n",
        "\n",
        "print(f\"Regressão (Teste) -> REMQ: baseline={rmse_base:.3f}, backward={rmse_bwd:.3f} | R2: baseline={r2_base:.3f}, backward={r2_bwd:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3daf0f1",
      "metadata": {
        "id": "a3daf0f1"
      },
      "source": [
        "\n",
        "## 6) Engenharia de atributos — Regressão **Polinomial**\n",
        "Criamos termos polinomiais de grau 2/3 para um subconjunto de colunas (ex.: `Longitude`, `Latitude`, `MedInc`). Avaliamos por CV e no teste.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d84fe02",
      "metadata": {
        "id": "5d84fe02"
      },
      "outputs": [],
      "source": [
        "\n",
        "poly_cols = ['Longitude', 'Latitude', 'MedInc']\n",
        "deg = 2  # altere para 3 e compare\n",
        "\n",
        "pre_poly = ColumnTransformer([('poly', PolynomialFeatures(degree=deg, include_bias=False), poly_cols)],\n",
        "                             remainder='passthrough')\n",
        "\n",
        "pipe_poly = Pipeline([('pre', pre_poly), ('scaler', StandardScaler(with_mean=False)), ('est', LinearRegression())])\n",
        "\n",
        "cv_mse = -cross_val_score(pipe_poly, Xr_train, yr_train, cv=cv_reg, scoring='neg_root_mean_squared_error')\n",
        "print(f\"CV REMQ (polinomial grau {deg}):\", cv_mse.mean().round(3))\n",
        "\n",
        "pipe_poly.fit(Xr_train, yr_train)\n",
        "pred_poly = pipe_poly.predict(Xr_test)\n",
        "rmse_poly = np.sqrt(mean_squared_error(yr_test, pred_poly))\n",
        "r2_poly = r2_score(yr_test, pred_poly)\n",
        "print(f\"Teste -> REMQ={rmse_poly:.3f} | R2={r2_poly:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac56117",
      "metadata": {
        "id": "cac56117"
      },
      "source": [
        "\n",
        "## 7) Interações/polynomial para **Logística**\n",
        "Termos de interação também podem ajudar na classificação (com cuidado). Exemplo: aplicar `PolynomialFeatures` em um subconjunto e comparar ROC-AUC por CV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f206037",
      "metadata": {
        "id": "3f206037"
      },
      "outputs": [],
      "source": [
        "\n",
        "cls_poly_cols = list(Xc_train.columns[:6])  # exemplo: usar 6 primeiras colunas\n",
        "deg_cls = 2\n",
        "\n",
        "pre_poly_cls = ColumnTransformer([('poly', PolynomialFeatures(degree=deg_cls, include_bias=False), cls_poly_cols)],\n",
        "                                remainder='passthrough')\n",
        "\n",
        "log_poly = Pipeline([('pre', pre_poly_cls), ('scaler', StandardScaler(with_mean=False)), ('est', LogisticRegression(max_iter=1000))])\n",
        "\n",
        "cv_poly_cls = cross_val_score(log_poly, Xc_train, yc_train, cv=cv_cls, scoring='accuracy')\n",
        "print(f\"Classificação (CV Accuracy) com termos polinomiais (grau {deg_cls}): {cv_poly_cls.mean():.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}